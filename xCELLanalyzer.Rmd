---
title: "xCELLanalyzer - Data Analysis"
output:
  word_document: default
  html_notebook: default
  html_document: default
  pdf_document: default
---

### Processing and analysis of the impedance data generated for *xCellAnalyze: A Framework for the Analysis of Cellular Impedance Measurements for Mode of Action Discovery*

N.B.: Datasets 1 to 10 were generated on xCelligence machine A and datasets 11 to 16 were generated on xCelligence machine B

Load libraries

```{r load libraries, message=FALSE, warning=FALSE}
library("tidyverse")
library("stringr")
library("knitr")
library("gtools")
library("gplots")
library("dendsort")
library(pheatmap)
library(RColorBrewer)

```

#### Source the xCellAnalyze functions:

* __read_xcell function:__
reads the tab-delimeted data exported from the RTCA Software Version 1.2. If the naming conventions are followed, only the global my_filepath varible and the experiment ID are used as arguments to the function.
The cryptic column names generated by the export from the RTCA Software are fixed to only contain the well identifier (of the E-plate).
To match the well labels with the compound IDs an _anno.txt is read containing the annotation of the wells for the appropriate experiment. The column names are then replaced with the corresponding compound IDs.

* __edit_df:__

* __normalize_xcell:__

* __do_median_polish:__

* __calculate_median_curves:__

* __normalize_dmso:__

* __smooth_splines:__


```{r}
source("xCell_functions.R")

```

#### Process the 12 xCelligence runs with the xcell_process_data.R script.
--> write down what the script does, comment on median polish and removal of outliers

```{r, message=FALSE, warning=FALSE, include=FALSE}
source("xcell_process_data.R")
```



#### Combine all matrices containing the normalized median TCRP data from each run in one big matrix and reorder the column names alphabetically


```{r}
median.combined <- cbind(xcell_median_norm_1, xcell_median_norm_2, xcell_median_norm_3,
                         xcell_median_norm_4, xcell_median_norm_5, xcell_median_norm_6,
                         xcell_median_norm_7, xcell_median_norm_8, xcell_median_norm_9,
                         xcell_median_norm_10, xcell_median_norm_11, xcell_median_norm_12,
                         xcell_median_norm_13, xcell_median_norm_14, xcell_median_norm_15,
                         xcell_median_norm_16)


for (i in 1 : length(median.combined[1,])){
  temp <- unlist(strsplit(toString(colnames(median.combined)[i]), "_"))
  colnames(median.combined)[i] <- paste(temp[2], temp[1], sep="_")
}


median.combined.ordered <- median.combined[,mixedorder(colnames(median.combined))]



```

#### Generate an analogous matrix without the DMSO normalization

```{r}
median.combined_notnorm <- cbind(
                         xcell_median_notnorm_1, xcell_median_notnorm_2, xcell_median_notnorm_3,
                         xcell_median_notnorm_4, xcell_median_notnorm_5, xcell_median_notnorm_6,
                         xcell_median_notnorm_7, xcell_median_notnorm_8, xcell_median_notnorm_9,
                         xcell_median_notnorm_10, xcell_median_notnorm_11, xcell_median_notnorm_12,
                         xcell_median_notnorm_13, xcell_median_notnorm_14, xcell_median_notnorm_15,
                         xcell_median_notnorm_16)

for (i in 1 : length(median.combined_notnorm[1,])){
  temp <- unlist(strsplit(toString(colnames(median.combined_notnorm)[i]), "_"))
  colnames(median.combined_notnorm)[i] <- paste(temp[2], temp[1], sep="_")
}

median.combined_notnorm.ordered <- median.combined_notnorm[,mixedorder(colnames(median.combined_notnorm))]

```


#### Calculate smoothing splines


```{r}
newrownames <- read.delim("newrownames.txt", header=F, stringsAsFactors = FALSE)

#smoothing splines with new rownames
median.sp<-matrix(ncol=22, nrow=220)
row.names(median.sp)<-newrownames$V1
t<-rownames(median.combined.ordered)
t<-as.numeric(t)

i<-0
repeat{
  i<-i+1
  temp<-smooth.spline(x=t, y= median.combined.ordered[,i], nknots=20)
  median.sp[i,]<-temp$fit$coef
  if (i==220) break
}

#analogous for the data without local normalization
median.sp.notnorm<-matrix(ncol=22, nrow=220)
row.names(median.sp.notnorm)<-newrownames$V1
t<-rownames(median.combined_notnorm.ordered)
t<-as.numeric(t)

i<-0
repeat{
  i<-i+1
  temp<-smooth.spline(x=t, y= median.combined_notnorm.ordered[,i], nknots=20)
  median.sp.notnorm[i,]<-temp$fit$coef
  if (i==220) break
}

```

#### Calculate score for biological replicates

The goal is to evalutate reproducibility for each compound and to judge which compounds are well reproducible and which not so much. For this purpose a rank-based score is calculated for each compound, the closer to one the better.
In addition an overall score is calculated, which is a single number to judge how the experiments and the data analysis overall performed. Again it is a rank-based score, the closer to one the better.

Here we also want to optimize certain parameters for the data analysis, namely the distance meassure and data scaling / centering.

Distance measures (source: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/dist.html)
compared are (written for two vectors x and y):

euclidean:
Usual distance between the two vectors (2 norm aka L_2), sqrt(sum((x_i - y_i)^2)).

maximum:
Maximum distance between two components of x and y (supremum norm)

manhattan:
Absolute distance between the two vectors (1 norm aka L_1).

Scaling and Centering of the matrices using the scale function of R base:
R documentation:
"The value of center determines how column centering is performed. If center is a numeric vector with length equal to the number of columns of x, then each column of x has the corresponding value from center subtracted from it. If center is TRUE then centering is done by subtracting the column means of x from their corresponding columns, and if center is FALSE, no centering is done.

The value of scale determines how column scaling is performed (after centering). If scale is a numeric vector with length equal to the number of columns of x, then each column of x is divided by the corresponding value from scale. If scale is TRUE then scaling is done by dividing the (centered) columns of x by their root-mean-square, and if scale is FALSE, no scaling is done.

The root-mean-square for a column is obtained by computing the square-root of the sum-of-squares of the non-missing values in the column divided by the number of non-missing values minus one."

```{r, echo=TRUE}
#criterion how the overall procedure scored 
res <- score1.function(median.sp, "euclidean")
euclidean <- sum(res$normscore)/i

median.sp.scaled <- scale(median.sp, center = FALSE, scale = TRUE)
res <- score1.function(median.sp.scaled, "euclidean")
euclidean.scaled <- sum(res$normscore)/i 

median.sp.centered.scaled <- scale(median.sp, center = TRUE, scale = TRUE)
res <- score1.function(median.sp.centered.scaled, "euclidean")
euclidean.centered.scaled <- sum(res$normscore)/i 
###

res <- score1.function(median.sp, "maximum")
maximum <- sum(res$normscore)/i

res <- score1.function(median.sp.scaled, "maximum")
maximum.scaled <- sum(res$normscore)/i 

res <- score1.function(median.sp.centered.scaled, "maximum")
maximum.centered.scaled <- sum(res$normscore)/i
###

res <- score1.function(median.sp, "manhattan")
manhattan <- sum(res$normscore)/i

res <- score1.function(median.sp.scaled, "manhattan")
manhattan.scaled <- sum(res$normscore)/i

res <- score1.function(median.sp.centered.scaled, "manhattan")
manhattan.centered.scaled <- sum(res$normscore)/i
###

not_scaled <- c(euclidean, maximum, manhattan)
scaled <- c(euclidean.scaled, maximum.scaled, manhattan.scaled)
cent_scaled <- c(euclidean.centered.scaled, maximum.centered.scaled, manhattan.centered.scaled)

tab1 <- rbind(not_scaled, scaled, cent_scaled)
colnames(tab1) <- c("euclidean", "maximum", "manhattan")
tab1 <- as.data.frame(tab1)

#grid.table(round(tab1, 3))
kable(round(tab1, 3), caption = "scores for 5 distance measures +/- scaling and centering")
```
Result: Euclidean distance with scaled (not centred) data performed best.
Mögliche Ergänzung/Erweiterung: correlation nehmen: Pearson, Spearman, Kendall

#alternative: use correlation
mydata <- t(median.sp.scaled.centered)
samples.cor.spearman <- cor(mydata,use="pairwise.complete.obs",  method="spearman")
samples.cor.spearman.dist <- as.dist(1-samples.cor.spearman)
my.distmat <- as.matrix(samples.cor.spearman.dist)
samples.tree <- hclust(samples.cor.spearman.dist,method='average')



1-cor is not optimal for hierarchical clustering:
http://research.stowers.org/mcm/efg/R/Visualization/cor-cluster/index.htm
hclust with a dissimilarity measure 1-Abs(Correlation) works fine.


Now we want to investigate if the local normalization with the TCRP from DMSO treated cells for each run lead to an improvement of reproducibility. 

```{r}
median.sp.notnorm.scaled <- scale(median.sp.notnorm, center = FALSE, scale = TRUE)
res <- score1.function(median.sp.notnorm.scaled, "euclidean")
euclidean.scaled.notnorm <- sum(res$normscore)/i 

median.sp.scaled <- scale(median.sp, center = FALSE, scale = TRUE)
res <- score1.function(median.sp.scaled, "euclidean")
euclidean.scaled <- sum(res$normscore)/i 
cat(paste0("without local normalization: ", round(euclidean.scaled.notnorm, 3), "\nwith local normalization: ", round(euclidean.scaled,3)))

```
Clearly the local normalization has lead to a strong improvement: 0.476 >> 0.266.


Evaluate reproducibility of biological replicates group-wise, calculate a score for each group of replicates
```{r}
res <- score1.function(median.sp, "euclidean")
groupmatch <- read.delim("groupmatch.txt", header=F)$V1
group.score <- c()

for(i in 1:length(groupmatch)){
  
  ma <- grep(groupmatch[i], res$rep)
  gscore <- sum(res$normscore[ma])/length(ma)
  group.score <- c(group.score, gscore)
  
}
group.result <- data.frame(groupmatch,group.score)
group.result <- group.result[order(-group.result$group.score),]
kable(group.result)

```


What we can do now is to use the score calculated for each biological replicate and define a threshold to remove replicates which are outliers. And then calculate the groupwise scores and the overall score again to check the improvement.


```{r}
#Filter reference set
#normailzed score < 0.1 is defined as outlier
res <- score1.function(median.sp, "euclidean")

my_hitlist <- res$rep[res$normscore < 0.1]
my_outliers <- c()

for (i in 1: length(my_hitlist)) {
temp <- unlist(strsplit(toString(my_hitlist[i]), "_"))
new_name <- paste0(temp[2], "_", temp[3])
my_outliers <- c(my_outliers, new_name)
}

print(my_outliers)

```



Plot the biological replicates of each compound (generated by the medians of the technical replicates on each E-plate) with and without local normalization.


```{r}
par(mfrow = c(1,2))
i<-0
repeat{
  i<-i+1
  
  
  ma<-grep(groupmatch[i], colnames(median.combined_notnorm.ordered)) 
  z<-median.combined_notnorm.ordered[,ma]
  z<-as.data.frame(z)
  z<-z[,mixedorder(names(z))]
  
  
  plot(x=rownames(z), y=z[,1], type="l", col="blue", main = "not normalized",
       cex.main=0.8, ylim=c(0,3), ylab="NCI", xlab="t [h]")
  
  
  
  myC <- length(ma)
  myColor <- c("green", "red", "black", "orange")
  
  for (n in 1:(myC-1))
  {
    lines(x=rownames(z), y=z[,n+1], type="l", col=myColor[n])
  }
  legend("topleft",legend=colnames(z),
         col= c("blue", myColor),pch=c(16,18),bty="n",ncol=1,cex=0.6,pt.cex=0.7)
  
  z<-median.combined.ordered[,ma]
  z<-as.data.frame(z)
  z<-z[,mixedorder(names(z))]
  
  
  plot(x=rownames(z), y=z[,1], type="l", col="blue", main = "normalized", 
       cex.main=0.8, ylim=c(-1.5,1.5), ylab="NCI", xlab="t [h]")
  
  
  for (n in 1:(myC-1))
  {
    lines(x=rownames(z), y=z[,n+1], type="l", col=myColor[n])
  }
  legend("topleft",legend=colnames(z),
         col= c("blue", myColor),pch=c(16,18),bty="n",ncol=1,cex=0.6,pt.cex=0.7)
  
  
  
  
  if (i==60) break
}

```

In the case where more than one biological replicate or one compound was round to be below the threshold, the one the deviates the most (judged by visual inspection of the plots) is removed from the data.

E.g. Archazolid: The replicate from run 13 is very close to the DMSO control, run 3 and 6 show more effect. Therefore ArchazolidB_13 is removed (see below).

```{r}
my_outliers_selected <- c("Apicularen_1", "ArchazolidB_13", "ArgyrinA_4", "Camptothecin_4", "Colchicine_16", "CruentarenA_14", "Doxorubicin_9", "EpothiloneB_13", "Etoposide_3", "GephyronicAcidA_9", "Indirubin3monoxime_3", "LY294002_2", "Methotrexate_1", "MG132_16", "Myriaporone_3", "Neopeltolide_16", "Nocodazole_15", "OkadaicAcid_2", "Oligomycin_15", "Oxamflatin_16", "PD169316_9", "RatjadonC_9", "Simvastatin_9", "Taxol_6", "Trichostatin_4", "Velcade_9", "Vioprolide_10")

ma <- match(my_outliers_selected, colnames(median.combined.ordered))
median.combined.edited <- median.combined.ordered[, -ma]


#calculate cubic smoothing splines

median.sp.edited<-matrix(ncol=22, nrow=193)
row.names(median.sp.edited)<-newrownames$V1[-ma]
t<-rownames(median.combined.edited)
t<-as.numeric(t)

i<-0
repeat{
  i<-i+1
  temp<-smooth.spline(x=t, y= median.combined.edited[,i], nknots=20)
  median.sp.edited[i,]<-temp$fit$coef
  if (i==193) break
}



median.sp.edited.scaled <- scale(median.sp.edited, center = FALSE, scale = TRUE)
res <- score1.function(median.sp.edited.scaled, "euclidean")
euclidean.scaled <- sum(res$normscore)/i 
cat(paste0("Euclidean.scaled after removal of outliers: ", round(euclidean.scaled, 3)))


group.score <- c()

for(i in 1:length(groupmatch)){
  
  ma <- grep(groupmatch[i], res$rep)
  gscore <- sum(res$normscore[ma])/length(ma)
  group.score <- c(group.score, gscore)
  
}
group.result <- data.frame(groupmatch,group.score)
group.result <- group.result[order(-group.result$group.score),]
kable(group.result)

```


With the improved data the medians of the biological replicates is calculated.

```{r}
#Calculate medians of medians
median.combined.median<-matrix(ncol=60, nrow=800)
colnames(median.combined.median)<-groupmatch
rownames(median.combined.median)<-row.names(median.combined.edited)
i<-0
repeat{
  i<-i+1
  
  name<-groupmatch[i]
  ma<-grep(groupmatch[i], colnames(median.combined.edited))
  z<-median.combined.edited[,ma]
  median.combined.median[,i]<-apply(z, 1, median) 
  
  if (i==60) break
}


#smoothing splines for median.combined
xmedian.combined<-matrix(ncol=22, nrow=60)
row.names(xmedian.combined)<-colnames(median.combined.median)
t<-rownames(median.combined.median)
t<-as.numeric(t)


i<-0
repeat{
  i<-i+1
  temp<-smooth.spline(x=t, y= median.combined.median[,i], nknots=20)
  xmedian.combined[i,]<-temp$fit$coef
  if (i==60) break
}


#Scaling
xmedian.combined.scaled <- scale(xmedian.combined, scale = TRUE, center = FALSE)
colnames(xmedian.combined.scaled) <- c("c1", "c2", "c3", "c4", "c5", "c6", "c7", "c8", "c9", "c10", "c11",
                                "c12", "c13", "c14", "c15", "c16", "c17", "c18", "c19",
                                "c20", "c21", "c22")

##distmat and hierarchical clustering
xmedian.combined.distmat <- dist(xmedian.combined.scaled, method = "euclidean")
xmedian.hclust.sorted <- dendsort(hclust(xmedian.combined.distmat, method = "complete"))
par(cex = 0.8)
plot(as.dendrogram(xmedian.hclust.sorted))


#heatmap
my_color = colorRampPalette(rev(brewer.pal(n = 10, name = "RdYlBu")))(100)
pheatmap(xmedian.combined.scaled, cluster_rows = xmedian.hclust.sorted, cluster_cols = TRUE, color = my_color, fontsize = 5.0)


```

Rank-based MoA prediction

```{r}
xmedian.combined.scaled <- scale(xmedian.combined)
mydistmat <- dist(xmedian.combined.scaled, method = "euclidean")
mydistmat <- as.matrix(mydistmat)
rank.predict <- matrix(ncol=60, nrow=60)
colnames(rank.predict) <- colnames(mydistmat)
rownames(rank.predict) <- c(1:60)

for (i in 1:60){
mydistmat.ordered <- mydistmat[order(mydistmat[,i]),]
rank.predict[,i] <- rownames(mydistmat.ordered) 
}

rank <- 1:59
rank.predict <- rank.predict[-1,]
rank.predict <- as.data.frame(cbind(rank, rank.predict))


write.csv2(rank.predict, "rankpredict.csv")
rank.predict
```